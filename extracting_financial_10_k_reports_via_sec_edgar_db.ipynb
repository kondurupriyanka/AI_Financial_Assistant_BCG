{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kondurupriyanka/AI_Financial_Assistant_BCG/blob/main/extracting_financial_10_k_reports_via_sec_edgar_db.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.040142,
          "end_time": "2020-11-02T15:35:42.154126",
          "exception": false,
          "start_time": "2020-11-02T15:35:42.113984",
          "status": "completed"
        },
        "tags": [],
        "id": "9Xbm8li-ZbDq"
      },
      "source": [
        "<h1 style=\"background-color: #ffd5cd; text-align: center\"> Extracting Financial 10-K Statements from SEC'S EDGAR Database</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.038114,
          "end_time": "2020-11-02T15:35:42.232767",
          "exception": false,
          "start_time": "2020-11-02T15:35:42.194653",
          "status": "completed"
        },
        "tags": [],
        "id": "30VedNWEZbDt"
      },
      "source": [
        "In this notebook, I aim to extract financial 10-K statements from the U.S. *Securities and Exchange Commision (SEC's)*, EDGAR database. In order to give an example, I would be extracting financial 10-K statements of 5 companies that belong to different sectors. These 5 stocks will constitute my diverse portfolio. This notebook is a work-in-progress. I would be leveraging the textual information from financial 10-K statements to perform NLP Analysis on them. This notebook covers extracting financial 10-K statements of companies and their preprocessing steps. The notebook is organized as follows:\n",
        "\n",
        "1. SEC EDGAR database overview.\n",
        "2. Fetching financial 10-K reports via SEC API\n",
        "3. Download 10-K statements\n",
        "4. Get documents.\n",
        "5. Get document types.\n",
        "6. Preprocessing 10-K documents\n",
        "    * Parsing via BeautifulSoup\n",
        "    * Lemmatization\n",
        "    * Stop-words removal\n",
        "7. Future work\n",
        "8. References"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2020-11-02T15:35:42.316139Z",
          "iopub.status.busy": "2020-11-02T15:35:42.315127Z",
          "iopub.status.idle": "2020-11-02T15:35:42.321923Z",
          "shell.execute_reply": "2020-11-02T15:35:42.320928Z"
        },
        "papermill": {
          "duration": 0.05169,
          "end_time": "2020-11-02T15:35:42.322105",
          "exception": false,
          "start_time": "2020-11-02T15:35:42.270415",
          "status": "completed"
        },
        "tags": [],
        "id": "9cc-Ksp-ZbDu"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.037905,
          "end_time": "2020-11-02T15:35:42.551675",
          "exception": false,
          "start_time": "2020-11-02T15:35:42.513770",
          "status": "completed"
        },
        "tags": [],
        "id": "e7xWzbVYZbDx"
      },
      "source": [
        "<h1 style=\"background-color: #ffd5cd; text-align: center\"> Fetching Financial 10-K Reports via SEC API</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.038216,
          "end_time": "2020-11-02T15:35:42.628025",
          "exception": false,
          "start_time": "2020-11-02T15:35:42.589809",
          "status": "completed"
        },
        "tags": [],
        "id": "21McsPovZbDx"
      },
      "source": [
        "When companies file their 10-K reports to the SEC, it is gathered in the EDGAR database and is publicly available for investors to download or search for company-wise filing reports, we need to submit an HTTPS request to the following REST Url:\n",
        "\n",
        "<p style=\"text-align:center; color:blue;\">https://www.sec.gov/cgi-bin/browse-edgar</p>\n",
        "\n",
        "To specify  the details of the report in which we are specifically interested in, we need to pass the following query parameters. To specify the details of the report in which we are specifically interested in, we need to pass the following query parameters:\n",
        "\n",
        "1. *CIK number (CIK)*: a unique numerical identifier assigned by the EDGAR system.\n",
        "2. *Report type (type)*:  type of financial report that we wish to query. Example 10-K, 10-Q, 14-K.\n",
        "3. *Prior-to date (dateb)*: EDGAR accepts a prior-to date that identifies the latest date in which we are interested.   \n",
        "4. *The number of reports (count)*: this quantity describes the number of filings up to the prior-to date.\n",
        "5. *Ownership (owner)*: The SEC requires filings from individuals who own significant amounts of the company’s stock. Setting the owner parameter to exclude, EDGAR won’t provide reports related to its director or officer ownership [1].\n",
        "\n",
        "As an example, to download Nike’s annual report before 2020, where Nike’s CIK number is 0000320187, and 10-K denotes the type of annual reports, we would form the EDGAR Url as follows :\n",
        "\n",
        "<p style=\"text-align:center; color:blue\"> https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320187&type=10-K&dateb=20200101&count=60&owner=exclude</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:35:42.710113Z",
          "iopub.status.busy": "2020-11-02T15:35:42.709363Z",
          "iopub.status.idle": "2020-11-02T15:35:42.712963Z",
          "shell.execute_reply": "2020-11-02T15:35:42.712246Z"
        },
        "papermill": {
          "duration": 0.046923,
          "end_time": "2020-11-02T15:35:42.713087",
          "exception": false,
          "start_time": "2020-11-02T15:35:42.666164",
          "status": "completed"
        },
        "tags": [],
        "id": "_vadxMoXZbDy"
      },
      "outputs": [],
      "source": [
        "cik_lookup = {\n",
        "    'AMZN': '0001018724',\n",
        "    'JNJ': '0000200406',\n",
        "    'MCD': '0000063908',\n",
        "    'PEP': '0000077476',\n",
        "    'WMT': '0000104169'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:35:42.795695Z",
          "iopub.status.busy": "2020-11-02T15:35:42.794807Z",
          "iopub.status.idle": "2020-11-02T15:35:55.804802Z",
          "shell.execute_reply": "2020-11-02T15:35:55.803966Z"
        },
        "papermill": {
          "duration": 13.053557,
          "end_time": "2020-11-02T15:35:55.804952",
          "exception": false,
          "start_time": "2020-11-02T15:35:42.751395",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc-mc9jmZbDy",
        "outputId": "d8a673de-abb0-473f-9495-2e321246c33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ratelimit\n",
            "  Downloading ratelimit-2.2.1.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ratelimit\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5895 sha256=80881b47966742f680a4f48ce74b817e09241b47138695ecc7ef90721d65c6af\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/5f/ba/e972a56dcbf5de9f2b7d2b2a710113970bd173c4dcd3d2c902\n",
            "Successfully built ratelimit\n",
            "Installing collected packages: ratelimit\n",
            "Successfully installed ratelimit-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ratelimit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2020-11-02T15:35:55.905256Z",
          "iopub.status.busy": "2020-11-02T15:35:55.904458Z",
          "iopub.status.idle": "2020-11-02T15:35:55.912110Z",
          "shell.execute_reply": "2020-11-02T15:35:55.911358Z"
        },
        "papermill": {
          "duration": 0.065025,
          "end_time": "2020-11-02T15:35:55.912239",
          "exception": false,
          "start_time": "2020-11-02T15:35:55.847214",
          "status": "completed"
        },
        "tags": [],
        "id": "7ko8C9TuZbDy"
      },
      "outputs": [],
      "source": [
        "from ratelimit import limits, sleep_and_retry\n",
        "import requests\n",
        "\n",
        "class SecAPI(object):\n",
        "    SEC_CALL_LIMIT = {'calls': 10, 'seconds': 1}\n",
        "\n",
        "    @staticmethod\n",
        "    @sleep_and_retry\n",
        "    @limits(calls=SEC_CALL_LIMIT['calls'] / 2, period=SEC_CALL_LIMIT['seconds'])\n",
        "    def _call_sec(url):\n",
        "        return requests.get(url)\n",
        "\n",
        "    def get(self, url):\n",
        "        return self._call_sec(url).text\n",
        "\n",
        "def print_ten_k_data(ten_k_data, fields, field_length_limit=50):\n",
        "    indentation = '  '\n",
        "\n",
        "    print('[')\n",
        "    for ten_k in ten_k_data:\n",
        "        print_statement = '{}{{'.format(indentation)\n",
        "        for field in fields:\n",
        "            value = str(ten_k[field])\n",
        "\n",
        "            if isinstance(value, str):\n",
        "                value_str = '\\'{}\\''.format(value.replace('\\n', '\\\\n'))\n",
        "            else:\n",
        "                value_str = str(value)\n",
        "\n",
        "            if len(value_str) > field_length_limit:\n",
        "                value_str = value_str[:field_length_limit] + '...'\n",
        "\n",
        "            print_statement += '\\n{}{}: {}'.format(indentation * 2, field, value_str)\n",
        "\n",
        "        print_statement += '},'\n",
        "        print(print_statement)\n",
        "    print(']')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:35:56.007008Z",
          "iopub.status.busy": "2020-11-02T15:35:56.006014Z",
          "iopub.status.idle": "2020-11-02T15:35:56.329917Z",
          "shell.execute_reply": "2020-11-02T15:35:56.330554Z"
        },
        "papermill": {
          "duration": 0.375004,
          "end_time": "2020-11-02T15:35:56.330712",
          "exception": false,
          "start_time": "2020-11-02T15:35:55.955708",
          "status": "completed"
        },
        "tags": [],
        "id": "TrxxC1_FZbDz"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "sec_api = SecAPI()\n",
        "\n",
        "def get_sec_data(cik, doc_type, start=0, count=60):\n",
        "    newest_pricing_date = pd.to_datetime('2018-08-01')\n",
        "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
        "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
        "        .format(cik, doc_type, start, count)\n",
        "    sec_data = sec_api.get(rss_url)\n",
        "    return sec_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.043032,
          "end_time": "2020-11-02T15:35:56.416830",
          "exception": false,
          "start_time": "2020-11-02T15:35:56.373798",
          "status": "completed"
        },
        "tags": [],
        "id": "l9ENQoaWZbDz"
      },
      "source": [
        "Hitting the EDGAR REST Url that we formed above would redirect us to a web page that contains tabular data related to the company’s type of filing document, document description, filing date, and file number. Figure 1 shows the EDGAR’s search result dashboard after hitting the REST Url formed above.\n",
        "\n",
        "![image.png](attachment:image.png)<br>\n",
        "<p style=\"text-align:center;\"><b>Figure 1: EDGAR search results for Nike’s 10-K documents prior-to 2020-01-01.</b>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.042731,
          "end_time": "2020-11-02T15:35:56.502464",
          "exception": false,
          "start_time": "2020-11-02T15:35:56.459733",
          "status": "completed"
        },
        "tags": [],
        "id": "rgbuTEFjZbDz"
      },
      "source": [
        "After fetching the web page as a response, we can perform web scraping with Python by leveraging the *BeautifulSoup* library and access the links that would help us download the 10-K filing reports. These document links will help us download the pure HTML version of the desired 10-K document, which we store in a dictionary against the corresponding stock’s CIK number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:35:56.600401Z",
          "iopub.status.busy": "2020-11-02T15:35:56.599218Z",
          "iopub.status.idle": "2020-11-02T15:35:56.602005Z",
          "shell.execute_reply": "2020-11-02T15:35:56.602567Z"
        },
        "papermill": {
          "duration": 0.057374,
          "end_time": "2020-11-02T15:35:56.602732",
          "exception": false,
          "start_time": "2020-11-02T15:35:56.545358",
          "status": "completed"
        },
        "tags": [],
        "id": "eaIJ5zXZZbDz"
      },
      "outputs": [],
      "source": [
        "def get_sec_data(cik, doc_type, start=0, count=60):\n",
        "    newest_pricing_data = pd.to_datetime('2020-01-01')\n",
        "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
        "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
        "        .format(cik, doc_type, start, count)\n",
        "    sec_data = sec_api.get(rss_url)\n",
        "    feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed\n",
        "    entries = [\n",
        "        (\n",
        "            entry.content.find('filing-href').getText(),\n",
        "            entry.content.find('filing-type').getText(),\n",
        "            entry.content.find('filing-date').getText())\n",
        "        for entry in feed.find_all('entry', recursive=False)\n",
        "        if pd.to_datetime(entry.content.find('filing-date').getText()) <= newest_pricing_data]\n",
        "\n",
        "    return entries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Assuming sec_api and SecAPI are defined and working correctly\n",
        "\n",
        "def get_sec_data(cik, doc_type, start=0, count=60):\n",
        "    newest_pricing_data = pd.to_datetime('2020-01-01')\n",
        "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
        "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
        "        .format(cik, doc_type, start, count)\n",
        "\n",
        "    # Assuming sec_api.get() returns a dictionary and the HTML content\n",
        "    # is stored under the key 'content'\n",
        "    sec_data = sec_api.get(rss_url)\n",
        "\n",
        "    # Extract the content from dictionary and use it as input for BeautifulSoup\n",
        "    sec_data_content = sec_data.get('content', '') # Get content, default to '' if not found\n",
        "\n",
        "    # Then pass the content string to BeautifulSoup\n",
        "    feed = BeautifulSoup(sec_data_content, 'xml').feed\n",
        "\n",
        "    entries = [\n",
        "        (\n",
        "            entry.content.find('filing-href').getText(),\n",
        "            entry.content.find('filing-type').getText(),\n",
        "            entry.content.find('filing-date').getText())\n",
        "        for entry in feed.find_all('entry', recursive=False)\n",
        "        if pd.to_datetime(entry.content.find('filing-date').getText()) <= newest_pricing_data]\n",
        "\n",
        "    return entries"
      ],
      "metadata": {
        "id": "-2R8ur2oZzBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.042643,
          "end_time": "2020-11-02T15:35:56.688647",
          "exception": false,
          "start_time": "2020-11-02T15:35:56.646004",
          "status": "completed"
        },
        "tags": [],
        "id": "onNTbv-lZbDz"
      },
      "source": [
        "Let's pull the list using the `get_sec_data` function, then display some of the results. For displaying some of the data, we'll use Amazon as an example."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sec_data(cik, doc_type, start=0, count=60):\n",
        "    newest_pricing_data = pd.to_datetime('2020-01-01')\n",
        "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
        "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
        "        .format(cik, doc_type, start, count)\n",
        "    sec_data = sec_api.get(rss_url)\n",
        "    # The issue is likely caused by forcing the encoding to 'ascii'.\n",
        "    # BeautifulSoup can usually auto-detect the encoding,\n",
        "    # or you can try 'utf-8' which is more versatile.\n",
        "    feed = BeautifulSoup(sec_data, 'xml').feed\n",
        "    entries = [\n",
        "        (\n",
        "            entry.content.find('filing-href').getText(),\n",
        "            entry.content.find('filing-type').getText(),\n",
        "            entry.content.find('filing-date').getText())\n",
        "        for entry in feed.find_all('entry', recursive=False)\n",
        "        if pd.to_datetime(entry.content.find('filing-date').getText()) <= newest_pricing_data]\n",
        "\n",
        "    return entries"
      ],
      "metadata": {
        "id": "DikqT7YbZ2K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_sec_data(cik, doc_type, start=0, count=60):\n",
        "    newest_pricing_data = pd.to_datetime('2020-01-01')\n",
        "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
        "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
        "        .format(cik, doc_type, start, count)\n",
        "\n",
        "    sec_data = sec_api.get(rss_url)\n",
        "\n",
        "    # Assuming sec_api.get() returns a string, use it directly as input for BeautifulSoup\n",
        "    feed = BeautifulSoup(sec_data, 'xml').feed\n",
        "\n",
        "    entries = [\n",
        "        (\n",
        "            entry.content.find('filing-href').getText(),\n",
        "            entry.content.find('filing-type').getText(),\n",
        "            entry.content.find('filing-date').getText())\n",
        "        for entry in feed.find_all('entry', recursive=False)\n",
        "        if pd.to_datetime(entry.content.find('filing-date').getText()) <= newest_pricing_data]\n",
        "\n",
        "    return entries"
      ],
      "metadata": {
        "id": "rZ4ZgNvHaL-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sec_data(cik, doc_type, start=0, count=10):\n",
        "    try:\n",
        "        # Fetch SEC filings\n",
        "        response = requests.get(f\"https://data.sec.gov/{cik}/{doc_type}\")\n",
        "        response.raise_for_status()\n",
        "        feed = BeautifulSoup(response.content, 'xml').feed\n",
        "\n",
        "        if feed is None:\n",
        "            print(f\"No data found for CIK: {cik}\")\n",
        "            return []\n",
        "\n",
        "        return [\n",
        "            (\n",
        "                entry.content.find('filing-type').getText(),\n",
        "                entry.content.find('filing-date').getText()\n",
        "            )\n",
        "            for entry in feed.find_all('entry', recursive=False)\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching SEC data for CIK {cik}: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "I9ShtZdNagG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "example_ticker = 'AMZN'\n",
        "sec_data = {}\n",
        "\n",
        "for ticker, cik in cik_lookup.items():\n",
        "    sec_data[ticker] = get_sec_data(cik, '10-K')\n",
        "\n",
        "pprint.pprint(sec_data[example_ticker][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcO8xGp1aQLg",
        "outputId": "832e5bf5-c3b1-4a0e-8caf-5e9fc763fb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching SEC data for CIK 0001018724: 403 Client Error: Forbidden for url: https://data.sec.gov/0001018724/10-K\n",
            "Error fetching SEC data for CIK 0000200406: 403 Client Error: Forbidden for url: https://data.sec.gov/0000200406/10-K\n",
            "Error fetching SEC data for CIK 0000063908: 403 Client Error: Forbidden for url: https://data.sec.gov/0000063908/10-K\n",
            "Error fetching SEC data for CIK 0000077476: 403 Client Error: Forbidden for url: https://data.sec.gov/0000077476/10-K\n",
            "Error fetching SEC data for CIK 0000104169: 403 Client Error: Forbidden for url: https://data.sec.gov/0000104169/10-K\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "raw_fillings_by_ticker = {}\n",
        "\n",
        "for ticker, data in sec_data.items():\n",
        "    raw_fillings_by_ticker[ticker] = {}\n",
        "    for index_url, file_type, file_date in tqdm(data, desc=f'Downloading {ticker} Filings', unit='filing'):\n",
        "        if file_type == '10-K':\n",
        "            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')\n",
        "\n",
        "            try:\n",
        "                response = sec_api.get(file_url)\n",
        "                raw_fillings_by_ticker[ticker][file_date] = response\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching {file_url} for {ticker}: {e}\")\n",
        "\n",
        "# Example document output\n",
        "if raw_fillings_by_ticker[example_ticker]:\n",
        "    print('Example Document:\\n\\n{}...'.format(\n",
        "        next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]\n",
        "    ))\n",
        "else:\n",
        "    print(f\"No data available for {example_ticker}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQNAGBtqaqwj",
        "outputId": "3f4a6a07-ba2a-45a7-f472-bd81501de293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading AMZN Filings: 0filing [00:00, ?filing/s]\n",
            "Downloading JNJ Filings: 0filing [00:00, ?filing/s]\n",
            "Downloading MCD Filings: 0filing [00:00, ?filing/s]\n",
            "Downloading PEP Filings: 0filing [00:00, ?filing/s]\n",
            "Downloading WMT Filings: 0filing [00:00, ?filing/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No data available for AMZN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.043724,
          "end_time": "2020-11-02T15:36:00.826923",
          "exception": false,
          "start_time": "2020-11-02T15:36:00.783199",
          "status": "completed"
        },
        "tags": [],
        "id": "DNgzvKimZbD0"
      },
      "source": [
        "<h1 style=\"background-color: #ffd5cd; text-align: center\"> Download 10-K Statements</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.09296,
          "end_time": "2020-11-02T15:38:13.866631",
          "exception": false,
          "start_time": "2020-11-02T15:38:13.773671",
          "status": "completed"
        },
        "tags": [],
        "id": "W0MN5xgEZbD0"
      },
      "source": [
        "<h1 style=\"background-color: #ffd5cd; text-align: center\"> Get Documents</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.095094,
          "end_time": "2020-11-02T15:38:14.055026",
          "exception": false,
          "start_time": "2020-11-02T15:38:13.959932",
          "status": "completed"
        },
        "tags": [],
        "id": "r3rkxd1XZbD0"
      },
      "source": [
        "With theses fillings downloaded, we want to break them into their associated documents. These documents are sectioned off in the fillings with the tags `<DOCUMENT>` for the start of each document and `</DOCUMENT>` for the end of each document. There's no overlap with these documents, so each `</DOCUMENT>` tag should come after the `<DOCUMENT>` with no `<DOCUMENT>` tag in between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:38:14.257454Z",
          "iopub.status.busy": "2020-11-02T15:38:14.256335Z",
          "iopub.status.idle": "2020-11-02T15:38:14.260526Z",
          "shell.execute_reply": "2020-11-02T15:38:14.259759Z"
        },
        "papermill": {
          "duration": 0.110548,
          "end_time": "2020-11-02T15:38:14.260654",
          "exception": false,
          "start_time": "2020-11-02T15:38:14.150106",
          "status": "completed"
        },
        "tags": [],
        "id": "sFLa-UItZbD0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def get_documents(text):\n",
        "    \"\"\"\n",
        "    Extract the documents from the text\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text with the document strings inside\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    extracted_docs : list of str\n",
        "        The document strings found in `text`\n",
        "    \"\"\"\n",
        "\n",
        "    final_docs = []\n",
        "    start_regex = re.compile(r'<DOCUMENT>')\n",
        "    end_regex = re.compile(r'</DOCUMENT>')\n",
        "\n",
        "    start_idx = [x.end() for x in re.finditer(start_regex, text)]\n",
        "    end_idx = [x.start() for x in re.finditer(end_regex, text)]\n",
        "\n",
        "    for start_i, end_i in zip(start_idx, end_idx):\n",
        "        final_docs.append(text[start_i:end_i])\n",
        "\n",
        "\n",
        "    return final_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:38:14.457621Z",
          "iopub.status.busy": "2020-11-02T15:38:14.456611Z",
          "iopub.status.idle": "2020-11-02T15:38:16.562158Z",
          "shell.execute_reply": "2020-11-02T15:38:16.561318Z"
        },
        "papermill": {
          "duration": 2.207988,
          "end_time": "2020-11-02T15:38:16.562330",
          "exception": false,
          "start_time": "2020-11-02T15:38:14.354342",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23zyA2C4ZbD0",
        "outputId": "0a804036-f309-44f4-819c-478a07de5894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Getting Documents from AMZN Fillings: 0filling [00:00, ?filling/s]\n",
            "Getting Documents from JNJ Fillings: 0filling [00:00, ?filling/s]\n",
            "Getting Documents from MCD Fillings: 0filling [00:00, ?filling/s]\n",
            "Getting Documents from PEP Fillings: 0filling [00:00, ?filling/s]\n",
            "Getting Documents from WMT Fillings: 0filling [00:00, ?filling/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "filling_documents_by_ticker = {}\n",
        "\n",
        "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
        "    filling_documents_by_ticker[ticker] = {}\n",
        "    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
        "        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
        "\n",
        "\n",
        "print('\\n\\n'.join([\n",
        "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
        "    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
        "    for doc_i, doc in enumerate(docs)][:3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.103972,
          "end_time": "2020-11-02T15:38:16.771798",
          "exception": false,
          "start_time": "2020-11-02T15:38:16.667826",
          "status": "completed"
        },
        "tags": [],
        "id": "Qakm_TlZZbD1"
      },
      "source": [
        "<h1 style=\"background-color: #ffd5cd; text-align: center\">Get Document Types</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.104321,
          "end_time": "2020-11-02T15:38:16.980503",
          "exception": false,
          "start_time": "2020-11-02T15:38:16.876182",
          "status": "completed"
        },
        "tags": [],
        "id": "_rnE7XoXZbD1"
      },
      "source": [
        "Now that we have all the documents, we want to find the 10-k form in this 10-k filing. The `get_document_type` function returns the type of document given. The document type is located on a line with the `<TYPE>` tag. For example, a form of type \"TEST\" would have the line `<TYPE>TEST`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:38:17.199341Z",
          "iopub.status.busy": "2020-11-02T15:38:17.198158Z",
          "iopub.status.idle": "2020-11-02T15:38:17.201778Z",
          "shell.execute_reply": "2020-11-02T15:38:17.201058Z"
        },
        "papermill": {
          "duration": 0.115949,
          "end_time": "2020-11-02T15:38:17.201924",
          "exception": false,
          "start_time": "2020-11-02T15:38:17.085975",
          "status": "completed"
        },
        "tags": [],
        "id": "ld0qbFheZbD1"
      },
      "outputs": [],
      "source": [
        "def get_document_type(doc):\n",
        "    \"\"\"\n",
        "    Return the document type lowercased\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    doc : str\n",
        "        The document string\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    doc_type : str\n",
        "        The document type lowercased\n",
        "    \"\"\"\n",
        "\n",
        "    # Regex explaination : Here I am tryng to do a positive lookbehind\n",
        "    # (?<=a)b (positive lookbehind) matches the b (and only the b) in cab, but does not match bed or debt.\n",
        "    # More reference : https://www.regular-expressions.info/lookaround.html\n",
        "\n",
        "    type_regex = re.compile(r'(?<=<TYPE>)\\w+[^\\n]+') # gives out \\w\n",
        "    type_idx = re.search(type_regex, doc).group(0).lower()\n",
        "    return type_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:38:17.463624Z",
          "iopub.status.busy": "2020-11-02T15:38:17.446229Z",
          "iopub.status.idle": "2020-11-02T15:38:17.472693Z",
          "shell.execute_reply": "2020-11-02T15:38:17.471764Z"
        },
        "papermill": {
          "duration": 0.166444,
          "end_time": "2020-11-02T15:38:17.472895",
          "exception": false,
          "start_time": "2020-11-02T15:38:17.306451",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F-fVHLdZbD1",
        "outputId": "f908c81a-1c6e-47aa-ba17-6dc523a466f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "ten_ks_by_ticker = {}\n",
        "\n",
        "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
        "    ten_ks_by_ticker[ticker] = []\n",
        "    for file_date, documents in filling_documents.items():\n",
        "        for document in documents:\n",
        "            if get_document_type(document) == '10-k':\n",
        "                ten_ks_by_ticker[ticker].append({\n",
        "                    'cik': cik_lookup[ticker],\n",
        "                    'file': document,\n",
        "                    'file_date': file_date})\n",
        "\n",
        "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.105254,
          "end_time": "2020-11-02T15:38:17.684790",
          "exception": false,
          "start_time": "2020-11-02T15:38:17.579536",
          "status": "completed"
        },
        "tags": [],
        "id": "suVyJSIOZbD1"
      },
      "source": [
        "<h1 style=\"background-color: #ffd5cd; text-align: center\">Preprocessing 10-K documents</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.104693,
          "end_time": "2020-11-02T15:38:17.894458",
          "exception": false,
          "start_time": "2020-11-02T15:38:17.789765",
          "status": "completed"
        },
        "tags": [],
        "id": "5QWJq4YmZbD1"
      },
      "source": [
        "<h2 style=\"background-color: #ffd5cd; text-align: center\">Parsing via BeautifulSoup</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:38:18.116182Z",
          "iopub.status.busy": "2020-11-02T15:38:18.115054Z",
          "iopub.status.idle": "2020-11-02T15:38:18.118729Z",
          "shell.execute_reply": "2020-11-02T15:38:18.117952Z"
        },
        "papermill": {
          "duration": 0.118745,
          "end_time": "2020-11-02T15:38:18.118884",
          "exception": false,
          "start_time": "2020-11-02T15:38:18.000139",
          "status": "completed"
        },
        "tags": [],
        "id": "IfIYEj8DZbD2"
      },
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = remove_html_tags(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:38:18.341243Z",
          "iopub.status.busy": "2020-11-02T15:38:18.340461Z",
          "iopub.status.idle": "2020-11-02T15:42:16.576292Z",
          "shell.execute_reply": "2020-11-02T15:42:16.576876Z"
        },
        "papermill": {
          "duration": 238.350043,
          "end_time": "2020-11-02T15:42:16.577064",
          "exception": false,
          "start_time": "2020-11-02T15:38:18.227021",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vzlMHvaZbD2",
        "outputId": "33fceeb4-8083-4a8b-f88a-5281462eeff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cleaning AMZN 10-Ks: 010-K [00:00, ?10-K/s]\n",
            "Cleaning JNJ 10-Ks: 010-K [00:00, ?10-K/s]\n",
            "Cleaning MCD 10-Ks: 010-K [00:00, ?10-K/s]\n",
            "Cleaning PEP 10-Ks: 010-K [00:00, ?10-K/s]\n",
            "Cleaning WMT 10-Ks: 010-K [00:00, ?10-K/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "        ten_k['file_clean'] = clean_text(ten_k['file'])\n",
        "\n",
        "\n",
        "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.148736,
          "end_time": "2020-11-02T15:42:16.876699",
          "exception": false,
          "start_time": "2020-11-02T15:42:16.727963",
          "status": "completed"
        },
        "tags": [],
        "id": "N24OAr3FZbD2"
      },
      "source": [
        "<h2 style=\"background-color: #ffd5cd; text-align: center\">Lemmatization</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:42:17.192122Z",
          "iopub.status.busy": "2020-11-02T15:42:17.191345Z",
          "iopub.status.idle": "2020-11-02T15:42:19.155296Z",
          "shell.execute_reply": "2020-11-02T15:42:19.154207Z"
        },
        "papermill": {
          "duration": 2.124494,
          "end_time": "2020-11-02T15:42:19.155456",
          "exception": false,
          "start_time": "2020-11-02T15:42:17.030962",
          "status": "completed"
        },
        "tags": [],
        "id": "Sj3acpSNZbD2"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    \"\"\"\n",
        "    Lemmatize words\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    words : list of str\n",
        "        List of words\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    lemmatized_words : list of str\n",
        "        List of lemmatized words\n",
        "    \"\"\"\n",
        "\n",
        "    wnl = WordNetLemmatizer()\n",
        "    lemmatized_words = [wnl.lemmatize(word, 'v') for word in words]\n",
        "\n",
        "    return lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:42:19.471789Z",
          "iopub.status.busy": "2020-11-02T15:42:19.470659Z",
          "iopub.status.idle": "2020-11-02T15:43:17.724545Z",
          "shell.execute_reply": "2020-11-02T15:43:17.725411Z"
        },
        "papermill": {
          "duration": 58.419649,
          "end_time": "2020-11-02T15:43:17.725590",
          "exception": false,
          "start_time": "2020-11-02T15:42:19.305941",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl7ynMRUZbD2",
        "outputId": "92bb95b5-9232-4a36-dff2-454896df98c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Lemmatize AMZN 10-Ks: 010-K [00:00, ?10-K/s]\n",
            "Lemmatize JNJ 10-Ks: 010-K [00:00, ?10-K/s]\n",
            "Lemmatize MCD 10-Ks: 010-K [00:00, ?10-K/s]\n",
            "Lemmatize PEP 10-Ks: 010-K [00:00, ?10-K/s]\n",
            "Lemmatize WMT 10-Ks: 010-K [00:00, ?10-K/s]\n"
          ]
        }
      ],
      "source": [
        "word_pattern = re.compile('\\w+')\n",
        "\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "        ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:43:18.163002Z",
          "iopub.status.busy": "2020-11-02T15:43:18.162176Z",
          "iopub.status.idle": "2020-11-02T15:43:18.167981Z",
          "shell.execute_reply": "2020-11-02T15:43:18.166897Z"
        },
        "papermill": {
          "duration": 0.243553,
          "end_time": "2020-11-02T15:43:18.168170",
          "exception": false,
          "start_time": "2020-11-02T15:43:17.924617",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHMy8C7nZbD3",
        "outputId": "02aea1f9-1b44-43da-d1de-80c0efb859a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.196757,
          "end_time": "2020-11-02T15:43:18.565755",
          "exception": false,
          "start_time": "2020-11-02T15:43:18.368998",
          "status": "completed"
        },
        "tags": [],
        "id": "8YEjc87CZbD3"
      },
      "source": [
        "<h2 style=\"background-color: #ffd5cd; text-align: center\">Stop-words Removal</h2>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QffF31thbOK4",
        "outputId": "88725b2c-249d-4c68-abf9-589e80b4524c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsuHF_InbObe",
        "outputId": "42d61508-d714-40f5-99ce-2880bc141f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords', download_dir='/content/nltk_data')  # Specify a local directory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMIyFQ9ubOeQ",
        "outputId": "11641866-83d6-4c82-b6e9-d8ade508ea99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define lemmatization function\n",
        "def lemmatize_words(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Load English stopwords and lemmatize them\n",
        "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
        "\n",
        "# Example input: Replace with your actual 10-K data structure\n",
        "ten_ks_by_ticker = {\n",
        "    \"AMZN\": [\n",
        "        {\"file_lemma\": [\"Amazon\", \"is\", \"a\", \"leading\", \"ecommerce\", \"platform\"]},\n",
        "        {\"file_lemma\": [\"The\", \"company\", \"focuses\", \"on\", \"customer\", \"satisfaction\"]}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Remove stopwords from each 10-K's lemmatized words\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    for ten_k in tqdm(ten_ks, desc=f'Remove Stop Words for {ticker} 10-Ks', unit='10-K'):\n",
        "        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]\n",
        "\n",
        "# Print processed data\n",
        "example_ticker = \"AMZN\"\n",
        "print(ten_ks_by_ticker[example_ticker])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpb9Vnwpbt0O",
        "outputId": "47fb7f1f-6f45-4e28-df59-a867d387e2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "Remove Stop Words for AMZN 10-Ks: 100%|██████████| 2/2 [00:00<00:00, 884.7810-K/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'file_lemma': ['Amazon', 'leading', 'ecommerce', 'platform']}, {'file_lemma': ['The', 'company', 'focuses', 'customer', 'satisfaction']}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-02T15:43:18.970262Z",
          "iopub.status.busy": "2020-11-02T15:43:18.969431Z",
          "iopub.status.idle": "2020-11-02T15:43:41.604556Z",
          "shell.execute_reply": "2020-11-02T15:43:41.603897Z"
        },
        "papermill": {
          "duration": 22.840732,
          "end_time": "2020-11-02T15:43:41.604710",
          "exception": false,
          "start_time": "2020-11-02T15:43:18.763978",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3o9clFFZbD3",
        "outputId": "cd225dd6-2259-4edb-a62b-4868c8d7b528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Remove Stop Words for AMZN 10-Ks: 100%|██████████| 2/2 [00:00<00:00, 2644.5810-K/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    file_lemma: '['Amazon', 'leading', 'ecommerce', 'platform']'},\n",
            "  {\n",
            "    file_lemma: '['The', 'company', 'focuses', 'customer', 'satisf...},\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
        "\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    for ten_k in tqdm(ten_ks, desc='Remove Stop Words for {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]\n",
        "\n",
        "\n",
        "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.236239,
          "end_time": "2020-11-02T15:43:43.073476",
          "exception": false,
          "start_time": "2020-11-02T15:43:42.837237",
          "status": "completed"
        },
        "tags": [],
        "id": "tiIFpPYNZbD4"
      },
      "source": [
        "<h1 style=\"background-color: #ffd5cd; text-align: center\">References</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.23447,
          "end_time": "2020-11-02T15:43:43.546279",
          "exception": false,
          "start_time": "2020-11-02T15:43:43.311809",
          "status": "completed"
        },
        "tags": [],
        "id": "Qv5srQdWZbD4"
      },
      "source": [
        "1. [GitHub - AI for Trading](https://github.com/purvasingh96/AI-for-Trading/tree/master/Term%202/Projects/Project%20-%205%20-%20NLP%20on%20Financial%20Statements)\n",
        "2. [SEC website](https://www.sec.gov/edgar/searchedgar/companysearch.html)\n",
        "3. [Udacity's Nanodegree materials](https://www.udacity.com/course/ai-for-trading--nd880)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 487.040865,
      "end_time": "2020-11-02T15:43:43.991289",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-11-02T15:35:36.950424",
      "version": "2.1.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}